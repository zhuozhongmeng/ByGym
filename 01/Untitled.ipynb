{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuozhongmeng/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-22 18:52:15.615695\n",
      "conv1.shape (?, 10, 10, 32)\n",
      "h_conv2.shape (?, 5, 5, 64)\n",
      "h_conv3.shape (?, 5, 5, 64)\n",
      "conv3_flat.shape (?, 1600)\n",
      "创建了一个网络\n",
      "Discrete(4)\n",
      "INFO:tensorflow:Restoring parameters from breakout8080/model.ckpt\n",
      "读取记忆\n",
      "14.0 [ 886 ]\n",
      "0 局得分： 14.0 分，最高 14.0 分，训练 0.9943630214205186 ，用时 0 秒,判断用 1.7420654296875 秒,总用： 7.945041179656982 秒,memoryusing 887 2019-04-22 18:52:25.207496\n",
      "-----------------------------------------------------------------------------15.0 [ 2154 ]11.0 [ 1428 ]17.0 [ 1502 ]9.0 [ 1109 ]5.0 [ 3499 ]19.0 [ 1727 ]16.0 [ 1843 ]0.0 [ 3499 ]35.0 [ 1687 ]13.0 [ 3303 ]\n",
      "10 局得分： 140.0 分，最高 35.0 分，训练 0.9990349708193558 ，用时 0 秒,判断用 41.920368909835815 秒,总用： 147.88220286369324 秒,memoryusing 22648 2019-04-22 18:54:54.448001\n",
      "-----------------------------------------------------------------------------12.0 [ 1391 ]16.0 [ 2914 ]18.0 [ 1398 ]25.0 [ 1360 ]14.0 [ 2138 ]19.0 [ 1864 ]11.0 [ 963 ]14.0 [ 1721 ]29.0 [ 2829 ]3.0 [ 3499 ]\n",
      "20 局得分： 161.0 分，最高 29.0 分，训练 0.9990043311594564 ，用时 0 秒,判断用 35.35821795463562 秒,总用： 94.06106996536255 秒,memoryusing 42735 2019-04-22 18:56:29.629350\n",
      "-----------------------------------------------------------------------------25.0 [ 3257 ]9.0 [ 2882 ]27.0 [ 2348 ]14.0 [ 829 ]0.0 [ 3499 ]31.0 [ 2954 ]12.0 [ 2130 ]34.0 [ 1148 ]17.0 [ 3499 ]31.0 [ 3273 ]\n",
      "30 局得分： 200.0 分，最高 34.0 分，训练 0.9988772310193968 ，用时 0 秒,判断用 47.222864866256714 秒,总用： 126.58330488204956 秒,memoryusing 50000 2019-04-22 18:58:37.399955\n",
      "-----------------------------------------------------------------------------0.0 [ 3499 ]19.0 [ 3499 ]13.0 [ 1410 ]0.0 [ 3499 ]0.0 [ 3499 ]4.0 [ 3499 ]2.0 [ 3499 ]0.0 [ 3499 ]30.0 [ 1462 ]7.0 [ 828 ]\n",
      "40 局得分： 75.0 分，最高 30.0 分，训练 0.9994326844661915 ，用时 0 秒,判断用 51.88580894470215 秒,总用： 138.7690508365631 秒,memoryusing 50000 2019-04-22 19:00:57.273457\n",
      "-----------------------------------------------------------------------------17.0 [ 3211 ]24.0 [ 2242 ]15.0 [ 1360 ]11.0 [ 754 ]14.0 [ 1186 ]10.0 [ 3499 ]8.0 [ 3499 ]9.0 [ 2670 ]14.0 [ 3065 ]26.0 [ 1358 ]\n",
      "50 局得分： 148.0 分，最高 26.0 分，训练 0.9990373676380502 ，用时 0 秒,判断用 42.55063080787659 秒,总用： 114.58903503417969 秒,memoryusing 50000 2019-04-22 19:02:53.523010\n",
      "-----------------------------------------------------------------------------8.0 [ 3499 ]19.0 [ 2853 ]17.0 [ 1531 ]32.0 [ 1316 ]24.0 [ 1196 ]17.0 [ 2234 ]20.0 [ 3134 ]20.0 [ 3320 ]18.0 [ 2138 ]23.0 [ 1103 ]\n",
      "60 局得分： 198.0 分，最高 32.0 分，训练 0.9987910808632579 ，用时 0 秒,判断用 44.19399309158325 秒,总用： 118.4115149974823 秒,memoryusing 50000 2019-04-22 19:04:53.326571\n",
      "-----------------------------------------------------------------------------26.0 [ 1174 ]0.0 [ 3499 ]13.0 [ 3153 ]11.0 [ 1979 ]13.0 [ 2462 ]37.0 [ 1814 ]24.0 [ 2175 ]5.0 [ 3499 ]17.0 [ 2271 ]21.0 [ 2101 ]\n",
      "70 局得分： 167.0 分，最高 37.0 分，训练 0.999047106102664 ，用时 0 秒,判断用 44.09337091445923 秒,总用： 116.18363809585571 秒,memoryusing 50000 2019-04-22 19:06:50.736916\n",
      "-----------------------------------------------------------------------------28.0 [ 2347 ]15.0 [ 3499 ]19.0 [ 2070 ]0.0 [ 3499 ]13.0 [ 1324 ]22.0 [ 2045 ]29.0 [ 1919 ]17.0 [ 2146 ]17.0 [ 3499 ]16.0 [ 1356 ]\n",
      "80 局得分： 176.0 分，最高 29.0 分，训练 0.999114447162014 ，用时 0 秒,判断用 44.90715718269348 秒,总用： 121.36042022705078 秒,memoryusing 50000 2019-04-22 19:08:53.288724\n",
      "-----------------------------------------------------------------------------1.0 [ 3499 ]28.0 [ 1521 ]45.0 [ 3182 ]5.0 [ 367 ]12.0 [ 2199 ]1.0 [ 3499 ]28.0 [ 2275 ]5.0 [ 3357 ]29.0 [ 1537 ]9.0 [ 3361 ]\n",
      "90 局得分： 163.0 分，最高 45.0 分，训练 0.9991131535453702 ，用时 0 秒,判断用 47.203978300094604 秒,总用： 128.4704191684723 秒,memoryusing 50000 2019-04-22 19:11:03.339997\n",
      "-----------------------------------------------------------------------------23.0 [ 2093 ]6.0 [ 3499 ]14.0 [ 1737 ]28.0 [ 1516 ]6.0 [ 3499 ]13.0 [ 1244 ]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-984ac4cd9aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-984ac4cd9aee>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m#print(\"start\",times)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mevn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#是否显示画面\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_with_4times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0;31m#print(\"action\",action,agent.get_action(state_with_4times))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-984ac4cd9aee>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnowreward\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;36m10011\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mget_action_time_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mget_action_time_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mget_action_time_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mget_action_time_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-984ac4cd9aee>\u001b[0m in \u001b[0;36mget_greedy_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m#state = np.reshape(state,[1,IMG_WIDTH,IMG_HEIGHT,IMG_TIME_LONG])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;31m#print(action,np.argmax(action))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0margmaxaction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4949\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4950\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4951\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# input all the support\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym as gym\n",
    "from collections import deque\n",
    "import random\n",
    "import cv2\n",
    "import time as pytime\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "# set static\n",
    "GAME = \"Breakout-v4\"\n",
    "MEMORYSIZE = 50000  # 保留样本大小\n",
    "Batch_size = 32  # 训练取样本大小\n",
    "GAMMA = 0.99  # 衰减率。伽马值，音译\n",
    "IMG_WIDTH = 80  # 图像宽度\n",
    "IMG_HEIGHT = 80  # 图像高度\n",
    "IMG_DEPTH = 1 #图像深度\n",
    "IMG_TIME_LONG = 4  # 图像时序长度\n",
    "INI_EPSILON = 1 #初始随机探索比例\n",
    "FINAL_EPSILON = 0.0001 #最终随机探索比例\n",
    "OBSEVER_TIMES = 65000 #一开始随便玩的次数\n",
    "TIMES_PER_ROUNDS = 3500 #限制每局最高动作数\n",
    "totalreward = 0\n",
    "# init Variable 定义及初始化一些全局变量\n",
    "view_total_reward = []  # 观察总得分分布\n",
    "view_best_reward = []  # 轮次最高分分布\n",
    "times_list=[]   #每局动作次数分布\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 定义一个图像处理方法，将图像切片变形成（40，40，1）\n",
    "def ImgProcess(state):\n",
    "    state1 = state[32:192, 0:160, 0:1]  # 截取有用信息，第一个方法是抽取第一个层图像，等于使用灰度图\n",
    "    small_state = cv2.resize(state1, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_AREA)  # 压缩到需要的画面大小\n",
    "    state3 = small_state[:,:,np.newaxis] #最后加一个维度，np.newaxis  新增维度。很字面的意思\n",
    "    return small_state\n",
    "    # 这里参考方法进行了图像的处理，调整了图像的曲线。\n",
    "\n",
    "\n",
    "def ColorMat2Binary(state):\n",
    "    height = state.shape[0]\n",
    "    width = state.shape[1]\n",
    "    nchannel = state.shape[2]\n",
    "    sHeight = int(height * 0.5)\n",
    "    sWidth = IMG_WIDTH\n",
    "\n",
    "    state_gray = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    _, state_binary = cv2.threshold(state_gray, 5, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    state_binarySmall = cv2.resize(state_binary, (sWidth, sHeight), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    cnn_inputImg = state_binarySmall[25:, :]\n",
    "    cnn_inputImg = cnn_inputImg.reshape((IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "    return cnn_inputImg\n",
    "\n",
    "\n",
    "\n",
    "def show_plt():\n",
    "    plt.plot(range(len(view_total_reward)),view_total_reward,'.')\n",
    "    plt.savefig(\"breakout8080/10round4.png\", dpi=300)\n",
    "    plt.close()\n",
    "    plt.plot(range(len(view_best_reward)),view_best_reward,'.')\n",
    "    plt.savefig(\"breakout8080/best4.png\",dpi=300)\n",
    "    plt.close()\n",
    "    plt.plot(range(len(times_list)), times_list, '.')\n",
    "    plt.savefig(\"breakout8080/timeslist4.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class DQN():\n",
    "\n",
    "    def __init__(self, evn):\n",
    "        self.action_dim = evn.action_space.n\n",
    "        self.session = tf.InteractiveSession()\n",
    "        self.creat_net()  # 一开始就初始化，创建一个网络出来先\n",
    "        self.memory = deque()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        self.m_reward = 0\n",
    "        self.insi = 0\n",
    "        self.training_time = 0\n",
    "        self.get_action_time = 0\n",
    "        self.get_action_times = 0\n",
    "        self.get_memory_time = 0\n",
    "        self.random_times = 0\n",
    "        self.m_times = 0\n",
    "        self.mm_reward = 1\n",
    "        self.temprandomtimes = 0\n",
    "        self.getaction = 10000\n",
    "        self.nowreward = 0\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def reload(self):\n",
    "        self.saver.restore(self.session, 'breakout8080/model.ckpt')\n",
    "        print(\"读取记忆\")\n",
    "\n",
    "    def save_weight(self):\n",
    "        self.saver.save(self.session, 'breakout8080/model.ckpt')\n",
    "        #print(\"保存成功,样本空间用量\",len(self.memory) * 100 / MEMORYSIZE, \"%\")\n",
    "\n",
    "    def show_randomtimes(self):\n",
    "        #print(\"训练占比\",self.m_times / (self.m_times + self.random_times))\n",
    "        self.temprandomtimes = self.m_times / (self.m_times + self.random_times)\n",
    "        self.random_times = 0\n",
    "        self.m_times = 0\n",
    "        return  self.temprandomtimes\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_weights(self, shape):\n",
    "        weight = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(weight)\n",
    "\n",
    "    def get_bias(self, shape):\n",
    "        bias = tf.constant(0.01, shape=shape)\n",
    "        return tf.Variable(bias)\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def creat_net(self):  # 创建tensorflow的图，用来直接逼出一个Q的网络价值函数\n",
    "        self.img_input = tf.placeholder(dtype=tf.float32, shape=[None, IMG_WIDTH, IMG_HEIGHT, IMG_TIME_LONG])\n",
    "        self.y_input = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "        self.action_input = tf.placeholder(dtype=tf.float32, shape=[None, self.action_dim])\n",
    "\n",
    "        w1 = self.get_weights([8, 8, 4, 32])\n",
    "        b1 = self.get_bias([32])\n",
    "        h_conv1 = tf.nn.relu(tf.nn.conv2d(self.img_input, w1, [1, 4, 4, 1], padding=\"SAME\") + b1)\n",
    "        conv1 = tf.nn.max_pool(h_conv1, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"SAME\")\n",
    "        print(\"conv1.shape\",conv1.shape)\n",
    "        w2 = self.get_weights([4, 4, 32, 64])\n",
    "        b2 = self.get_bias([64])\n",
    "        h_conv2 = tf.nn.relu(tf.nn.conv2d(conv1, w2, [1, 2, 2, 1], padding=\"SAME\") + b2)\n",
    "        print(\"h_conv2.shape\",h_conv2.shape)\n",
    "        #h_conv2 = tf.nn.max_pool(h_conv2, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"SAME\")\n",
    "        #print(\"h_conv2_after_max_pool.shape\", h_conv2.shape)\n",
    "\n",
    "        w3 = self.get_weights([3, 3, 64, 64])\n",
    "        b3 = self.get_bias([64])\n",
    "        h_conv3 = tf.nn.relu(tf.nn.conv2d(h_conv2, w3, [1, 1, 1, 1], padding=\"SAME\") + b3)\n",
    "        print(\"h_conv3.shape\", h_conv3.shape)\n",
    "        w_fc1 = self.get_weights([1600, 512])\n",
    "        b_fc1 = self.get_bias([512])\n",
    "        conv3_flat = tf.reshape(h_conv3, [-1,1600])\n",
    "        print(\"conv3_flat.shape\", conv3_flat.shape)\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(conv3_flat, w_fc1) + b_fc1)\n",
    "\n",
    "        w_fc2 = self.get_weights([512, self.action_dim])\n",
    "        b_fc2 = self.get_bias([self.action_dim])\n",
    "\n",
    "        self.Q_value = tf.matmul(h_fc1, w_fc2) + b_fc2  # 直到这里，拿到的只是一个图像的识别结果抽象，带action的二维矩阵 当作是价值函数\n",
    "        Q_action = tf.reduce_sum(tf.multiply(self.Q_value, self.action_input), reduction_indices=1)  # 这个是动作价值函数\n",
    "        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))  # y_input 就是最佳策略得分，就是回报，来自于马尔可夫过程结果，这里就是让输出不断的毕竟最佳策略得分\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(1e-6).minimize(self.cost)\n",
    "        print(\"创建了一个网络\")\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def training(self):\n",
    "        minibatch = random.sample(self.memory, Batch_size)  # 这里是利用随机库，在记忆中，随机抽取一定数量minisize = 10 的记忆。然后等待下一步使用。\n",
    "        mini_state = [data[0] for data in minibatch]\n",
    "        mini_action = [data[1] for data in minibatch]\n",
    "        mini_next_state = [data[2] for data in minibatch]\n",
    "        mini_reward = [data[3] for data in minibatch]\n",
    "        mini_done = [data[4] for data in minibatch]\n",
    "        total_Q = []\n",
    "        next_Q_value = self.Q_value.eval(feed_dict={self.img_input: mini_next_state})\n",
    "\n",
    "        for i in range(Batch_size):\n",
    "            if mini_done[i]:\n",
    "                total_Q.append(mini_reward[i])\n",
    "                # print(\"游戏失败\")\n",
    "            else:\n",
    "                total_Q.append(mini_reward[i] + GAMMA * np.max(next_Q_value[i]))\n",
    "                # print(\"记录贝尔曼，\",np.argmax(next_Q_value[i]))\n",
    "                # print(np.shape(total_Q))\n",
    "        self.optimizer.run(feed_dict={\n",
    "            self.img_input: mini_state,\n",
    "            self.action_input: mini_action,\n",
    "            self.y_input: total_Q\n",
    "        })\n",
    "        #writer = tf.summary.FileWriter(\"log\", self.session.graph)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_greedy_action(self, state):\n",
    "        #state = np.reshape(state,[1,IMG_WIDTH,IMG_HEIGHT,IMG_TIME_LONG])\n",
    "        action = self.Q_value.eval(feed_dict={self.img_input: [state]})[0]\n",
    "        #print(action,np.argmax(action))\n",
    "        argmaxaction = np.argmax(action)\n",
    "        #print(\"no[0]\",self.Q_value.eval(feed_dict={self.img_input: state}),np.argmax(self.Q_value.eval(feed_dict={self.img_input: state})))\n",
    "        return np.argmax(action)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        self.getaction += 1\n",
    "        if self.getaction > 10000:\n",
    "            self.getaction = 10000\n",
    "        #if self.get_action_times < 999999999:\n",
    "        #    self.get_action_times += 1\n",
    "       # random_area = 1 - self.get_action_times * 0.000000001\n",
    "        if random.random() > 1 - (self.getaction + self.nowreward /100) /10011:\n",
    "            get_action_time_start = pytime.time()\n",
    "            action = self.get_greedy_action(state)\n",
    "            get_action_time_end = pytime.time()\n",
    "            self.get_action_time += get_action_time_end - get_action_time_start\n",
    "            self.m_times += 1\n",
    "        else:\n",
    "            action = random.randint(0, self.action_dim - 1)\n",
    "            self.random_times += 1\n",
    "        return action\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def percieve(self, state, action, next_state, reward, done, now_times):\n",
    "\n",
    "        action_index = np.zeros(self.action_dim)\n",
    "        action_index[action] = 1\n",
    "        self.get_memory_time += 1\n",
    "        self.memory.append([state, action_index, next_state, reward, done, now_times])\n",
    "\n",
    "        if len(self.memory) > MEMORYSIZE:\n",
    "            self.memory.popleft()\n",
    "        if len(self.memory) > OBSEVER_TIMES:\n",
    "            if self.insi == 0:\n",
    "                print(\"试玩结束，开始训练\")\n",
    "                self.insi = 1\n",
    "\n",
    "            per_training_stare = pytime.time()\n",
    "            self.training()\n",
    "            per_training_end = pytime.time()\n",
    "\n",
    "            self.training_time += per_training_end - per_training_stare\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(datetime.datetime.now())\n",
    "    evn = gym.make(GAME)\n",
    "    agent = DQN(evn)\n",
    "    print(evn.action_space)\n",
    "    agent.reload()\n",
    "    round_reward = 0\n",
    "    best_reward = 0\n",
    "    round_10_reward = 0\n",
    "    round_time_start = pytime.time()  # --------------------------------------------获取本局开始时间\n",
    "    state_with_4times = None\n",
    "    state_with_4times = None\n",
    "\n",
    "    for rounds in range(100000000000000):\n",
    "\n",
    "        state = evn.reset()\n",
    "        #print(\"reset\",datetime.datetime.now())\n",
    "        state = ColorMat2Binary(state)\n",
    "        state_with_4times = np.stack((state, state, state, state), axis=2)\n",
    "        for times in range(TIMES_PER_ROUNDS):\n",
    "            #print(\"start\",times)\n",
    "            evn.render() #是否显示画面\n",
    "            action = agent.get_action(state_with_4times)\n",
    "            #print(\"action\",action,agent.get_action(state_with_4times))\n",
    "            next_state, reward, done, _ = evn.step(action)\n",
    "            #print(next_state, reward, done, _ )\n",
    "            next_state = ColorMat2Binary(next_state)\n",
    "            next_state = np.reshape(next_state, [IMG_WIDTH, IMG_HEIGHT, 1])\n",
    "            next_state_with_4times = np.append(next_state,state_with_4times[:, :, :3],  axis=2)  # 记录时序状态\n",
    "            agent.percieve(state_with_4times, action, next_state_with_4times, reward, done, times)\n",
    "            state_with_4times = next_state_with_4times #更新输入状态\n",
    "            round_reward += reward\n",
    "            round_10_reward += reward\n",
    "\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if round_reward > best_reward:\n",
    "            best_reward = round_reward\n",
    "        #print(done)\n",
    "        print(round_reward,'[',times,']',end='')\n",
    "        times_list.append(times)\n",
    "        round_reward = 0\n",
    "\n",
    "        if rounds % 10 == 0:\n",
    "            round_time_end = pytime.time()  # ------------------------------------------获取本局结束时间\n",
    "            print(\"\")\n",
    "            print(rounds, \"局得分：\", round_10_reward, \"分，最高\", best_reward, \"分，训练\",\n",
    "                  agent.show_randomtimes(), \"，用时\", agent.training_time, \"秒,判断用\", agent.get_action_time, \"秒,总用：\",\n",
    "                  round_time_end - round_time_start, \"秒,memoryusing\",len(agent.memory),datetime.datetime.now())\n",
    "            print('-----------------------------------------------------------------------------',end=\"\")\n",
    "            view_total_reward.append(round_10_reward)\n",
    "            view_best_reward.append(best_reward)\n",
    "            agent.nowreward = round_10_reward\n",
    "            round_10_reward = 0\n",
    "            agent.save_weight()\n",
    "            best_reward = 0\n",
    "            show_plt()\n",
    "            agent.get_action_time = 0\n",
    "            agent.training_time = 0\n",
    "            round_time_start = pytime.time()  # --------------------------------------------获取本局开始时间\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
